"""
ì˜¨ì‹¤ ìƒìœ¡ì§€í‘œ ë° ìˆ˜í™•ëŸ‰ ì˜ˆì¸¡ Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜
- ì›í´ë¦­: í™˜ê²½ ë°ì´í„° â†’ ìƒìœ¡ì§€í‘œ â†’ ìˆ˜í™•ëŸ‰ ì˜ˆì¸¡
- 1ë‹¨ê³„: í™˜ê²½ ë°ì´í„° â†’ ìƒìœ¡ì§€í‘œ ì˜ˆì¸¡
- 2ë‹¨ê³„: í™˜ê²½+ìƒìœ¡ ë°ì´í„° â†’ ìˆ˜í™•ëŸ‰ ì˜ˆì¸¡
"""

import streamlit as st
import os
import json
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import joblib
from datetime import datetime
import io

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ì˜¨ì‹¤ ìƒìœ¡Â·ìˆ˜í™• ì˜ˆì¸¡ ì‹œìŠ¤í…œ",
    page_icon="ğŸŒ±",
    layout="wide"
)

# ìƒìˆ˜ ì •ì˜
MODEL_FOLDER = "models"
KEY_FARM = "ë†ê°€ëª…"
KEY_DATE = "ì¼ì‹œ"

TRAIN_TARGETS = [
    "ê°œí™”ë§ˆë””", "ì°©ê³¼ë§ˆë””", "ìˆ˜í™•ë§ˆë””", "ì°©ê³¼ìˆ˜", "ì—´ë§¤ìˆ˜", "ì´ˆì¥",
    "ìƒì¥ê¸¸ì´", "ì—½ìˆ˜", "ì—½ì¥", "ì—½í­", "ì¤„ê¸°êµµê¸°", "í™”ë°©ë†’ì´"
]
HARVEST_TARGET = "ì£¼ë‹¹ëˆ„ì ìˆ˜í™•ìˆ˜"

# ==================== ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤ ====================
class SkewAwareScaler:
    """ì™œë„ ê¸°ë°˜ ìŠ¤ì¼€ì¼ëŸ¬"""
    def __init__(self, low_thr=0.3, high_thr=1.0, fillna="median", eps=1e-6):
        self.low_thr = low_thr
        self.high_thr = high_thr
        self.fillna = fillna
        self.eps = eps
        self.meta = {}
        self._scalers = {}

    def transform(self, df, cols):
        df = df.copy()
        if self.fillna == "median":
            for c in cols:
                df[c] = df[c].fillna(df[c].median())
        elif self.fillna == "mean":
            for c in cols:
                df[c] = df[c].fillna(df[c].mean())

        out = df.copy()
        for c in cols:
            info = self.meta[c]
            r = info["rule"]
            shift = info["shift"]
            sc = self._scalers[c]
            x = out[c].astype(float).values.copy()

            if r == "constant":
                out[c] = 0.0
                continue
            if r == "log_robust":
                x = np.log1p(x + shift)
            if sc is not None:
                x = sc.transform(x.reshape(-1, 1)).ravel()
            out[c] = x
        return out

    def inverse_transform(self, df_scaled, cols):
        out = df_scaled.copy()
        for c in cols:
            info = self.meta[c]
            r = info["rule"]
            shift = info["shift"]
            sc = self._scalers[c]
            x = out[c].astype(float).values.copy()

            if r != "constant" and sc is not None:
                x = sc.inverse_transform(x.reshape(-1, 1)).ravel()
            if r == "log_robust":
                x = np.expm1(x) - shift
            out[c] = x
        return out

# ==================== ëª¨ë¸ í´ë˜ìŠ¤ ====================
class GRURegVar(nn.Module):
    """ê°€ë³€ ê¸¸ì´ ì‹œí€€ìŠ¤ GRU ëª¨ë¸"""
    def __init__(self, in_dim, hidden=96, layers=2, out_dim=1, dropout=0.2):
        super().__init__()
        self.gru = nn.GRU(
            in_dim, hidden, num_layers=layers, batch_first=True,
            dropout=dropout if layers > 1 else 0.0
        )
        self.fc = nn.Linear(hidden, out_dim)

    def forward(self, x, lens):
        from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
        
        packed = pack_padded_sequence(x, lengths=lens.cpu(), batch_first=True, enforce_sorted=True)
        out, _ = self.gru(packed)
        out, _ = pad_packed_sequence(out, batch_first=True)
        
        idx = (lens - 1).view(-1, 1, 1).expand(out.size(0), 1, out.size(2))
        last = out.gather(1, idx).squeeze(1)
        
        return self.fc(last)

# ==================== ìºì‹±ëœ í•¨ìˆ˜ë“¤ ====================
@st.cache_data
def sanitize_datetime_col(df, date_col):
    """ë‚ ì§œ ì»¬ëŸ¼ ì •ì œ"""
    df = df.copy()
    df[date_col] = pd.to_datetime(df[date_col], errors="coerce")
    df = df.dropna(subset=[date_col])
    return df

@st.cache_resource
def load_model_and_scalers(target, model_root):
    """ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ (ìºì‹±)"""
    target_dir = os.path.join(model_root, target)
    
    if not os.path.exists(target_dir):
        raise FileNotFoundError(f"ëª¨ë¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {target_dir}")

    # ë©”íƒ€ë°ì´í„°
    with open(os.path.join(target_dir, "model_meta.json"), "r", encoding="utf-8") as f:
        meta = json.load(f)

    # ìŠ¤ì¼€ì¼ëŸ¬
    scaler_x = joblib.load(os.path.join(target_dir, "scaler_x.pkl"))
    scaler_y = joblib.load(os.path.join(target_dir, "scaler_y.pkl"))

    hp = meta["hparams_best"]
    feature_cols = meta["features"]

    # ëª¨ë¸
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    weight_path = os.path.join(target_dir, "best_model.pt")
    checkpoint = torch.load(weight_path, map_location=device)

    try:
        hidden_size = checkpoint['gru.weight_hh_l0'].shape[1]
        num_layers = max([int(k.split('_l')[1].split('.')[0]) for k in checkpoint.keys()
                         if 'gru.weight_ih_l' in k]) + 1
    except:
        hidden_size = hp.get("HIDDEN", 96)
        num_layers = hp.get("LAYERS", 2)

    dropout = hp.get("DROPOUT", 0.2)

    model = GRURegVar(
        in_dim=len(feature_cols),
        hidden=hidden_size,
        layers=num_layers,
        out_dim=1,
        dropout=dropout
    ).to(device)

    model.load_state_dict(checkpoint)
    model.eval()

    return model, scaler_x, scaler_y, hp, feature_cols, device

def predict_daily_values(df_env, target, model, scaler_x, scaler_y, hp, feature_cols, device):
    """ì¼ë³„ ì˜ˆì¸¡"""
    df_env_scaled = df_env.copy()
    df_env_scaled[feature_cols] = scaler_x.transform(df_env[feature_cols], feature_cols)

    min_days = hp["MIN_DAYS"]
    max_days = hp["MAX_DAYS"]
    predictions = []

    for farm, group in df_env_scaled.groupby(KEY_FARM):
        group = group.sort_values(KEY_DATE).set_index(KEY_DATE)

        for current_date in group.index:
            start_date = current_date - pd.Timedelta(days=max_days)
            end_date = current_date - pd.Timedelta(days=1)

            if end_date < start_date:
                continue

            date_range = pd.date_range(start_date, end_date, freq="D")
            available_dates = group.index.intersection(date_range)

            if len(available_dates) < min_days:
                continue

            sequence = group.loc[available_dates, feature_cols]

            if sequence.isna().any().any():
                continue

            if len(sequence) > max_days:
                sequence = sequence.iloc[-max_days:]

            x = torch.tensor(sequence.values, dtype=torch.float32).unsqueeze(0).to(device)
            lens = torch.tensor([len(sequence)], dtype=torch.long).to(device)

            with torch.no_grad():
                y_hat = model(x, lens).cpu().numpy().reshape(-1)

            pred_value = scaler_y.inverse_transform(
                pd.DataFrame({target: [y_hat[0]]}),
                [target]
            )[target].iloc[0]

            predictions.append({
                KEY_FARM: farm,
                KEY_DATE: current_date,
                target: float(pred_value)
            })

    return pd.DataFrame(predictions)

# ==================== ì˜ˆì¸¡ í•¨ìˆ˜ë“¤ ====================
def process_step1(df_env, df_measured=None):
    """1ë‹¨ê³„: ìƒìœ¡ì§€í‘œ ì˜ˆì¸¡"""
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    all_predictions = []
    total = len(TRAIN_TARGETS)

    for idx, target in enumerate(TRAIN_TARGETS):
        try:
            status_text.text(f"ì˜ˆì¸¡ ì¤‘: {target} ({idx+1}/{total})")
            
            model, scaler_x, scaler_y, hp, feature_cols, device = load_model_and_scalers(target, MODEL_FOLDER)
            df_pred = predict_daily_values(df_env, target, model, scaler_x, scaler_y, hp, feature_cols, device)
            all_predictions.append(df_pred)
            
            progress_bar.progress((idx + 1) / total)
        except Exception as e:
            st.warning(f"{target} ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")
            continue

    if len(all_predictions) == 0:
        raise Exception("ëª¨ë“  ì˜ˆì¸¡ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

    status_text.text("ê²°ê³¼ ë³‘í•© ì¤‘...")
    
    # ë³‘í•©
    merged = all_predictions[0]
    for df in all_predictions[1:]:
        merged = merged.merge(df, on=[KEY_FARM, KEY_DATE], how="outer")

    result = df_env.merge(merged, on=[KEY_FARM, KEY_DATE], how="left")

    # ì‹¤ì¸¡ê°’ ë³‘í•©
    if df_measured is not None:
        for target in TRAIN_TARGETS:
            if target in df_measured.columns:
                measured_data = df_measured[[KEY_FARM, KEY_DATE, target]].copy()
                result = result.merge(measured_data, on=[KEY_FARM, KEY_DATE], how="left", suffixes=('_pred', ''))
                
                if f'{target}_pred' in result.columns:
                    result[target] = result[target].fillna(result[f'{target}_pred'])
                    result.drop(columns=[f'{target}_pred'], inplace=True)

    result = result.sort_values([KEY_FARM, KEY_DATE])
    
    progress_bar.progress(1.0)
    status_text.text("ì™„ë£Œ!")
    
    return result

def process_step2(df_combined):
    """2ë‹¨ê³„: ìˆ˜í™•ëŸ‰ ì˜ˆì¸¡"""
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    status_text.text("ê²°ì¸¡ì¹˜ ì œê±° ì¤‘...")
    original_rows = len(df_combined)
    df_clean = df_combined.dropna()
    removed = original_rows - len(df_clean)
    
    if len(df_clean) == 0:
        raise Exception("ê²°ì¸¡ì¹˜ ì œê±° í›„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    st.info(f"ê²°ì¸¡ì¹˜ {removed}í–‰ ì œê±°ë¨")
    progress_bar.progress(0.3)
    
    status_text.text("ìˆ˜í™•ëŸ‰ ì˜ˆì¸¡ ì¤‘...")
    model, scaler_x, scaler_y, hp, feature_cols, device = load_model_and_scalers(HARVEST_TARGET, MODEL_FOLDER)
    
    progress_bar.progress(0.6)
    df_harvest = predict_daily_values(df_clean, HARVEST_TARGET, model, scaler_x, scaler_y, hp, feature_cols, device)
    
    status_text.text("ê²°ê³¼ ë³‘í•© ì¤‘...")
    result = df_clean.merge(df_harvest, on=[KEY_FARM, KEY_DATE], how="left")
    result = result.sort_values([KEY_FARM, KEY_DATE])
    
    progress_bar.progress(1.0)
    status_text.text("ì™„ë£Œ!")
    
    return result, removed

def process_direct(df_env, df_measured=None):
    """í™˜ê²½ â†’ ìƒìœ¡ â†’ ìˆ˜í™•"""
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # Step 1: ìƒìœ¡ì§€í‘œ
    status_text.text("ìƒìœ¡ì§€í‘œ ì˜ˆì¸¡ ì¤‘...")
    all_predictions = []
    total = len(TRAIN_TARGETS)

    for idx, target in enumerate(TRAIN_TARGETS):
        try:
            model, scaler_x, scaler_y, hp, feature_cols, device = load_model_and_scalers(target, MODEL_FOLDER)
            df_pred = predict_daily_values(df_env, target, model, scaler_x, scaler_y, hp, feature_cols, device)
            all_predictions.append(df_pred)
            progress_bar.progress(0.5 * (idx + 1) / total)
        except:
            continue

    merged = all_predictions[0]
    for df in all_predictions[1:]:
        merged = merged.merge(df, on=[KEY_FARM, KEY_DATE], how="outer")

    df_combined = df_env.merge(merged, on=[KEY_FARM, KEY_DATE], how="left")

    # ì‹¤ì¸¡ê°’ ë³‘í•©
    if df_measured is not None:
        for target in TRAIN_TARGETS:
            if target in df_measured.columns:
                measured_data = df_measured[[KEY_FARM, KEY_DATE, target]].copy()
                df_combined = df_combined.merge(measured_data, on=[KEY_FARM, KEY_DATE], how="left", suffixes=('_pred', ''))
                if f'{target}_pred' in df_combined.columns:
                    df_combined[target] = df_combined[target].fillna(df_combined[f'{target}_pred'])
                    df_combined.drop(columns=[f'{target}_pred'], inplace=True)

    status_text.text("ê²°ì¸¡ì¹˜ ì œê±° ì¤‘...")
    progress_bar.progress(0.6)
    
    original_rows = len(df_combined)
    df_clean = df_combined.dropna()
    removed = original_rows - len(df_clean)
    
    # Step 2: ìˆ˜í™•ëŸ‰
    status_text.text("ìˆ˜í™•ëŸ‰ ì˜ˆì¸¡ ì¤‘...")
    progress_bar.progress(0.75)
    
    model, scaler_x, scaler_y, hp, feature_cols, device = load_model_and_scalers(HARVEST_TARGET, MODEL_FOLDER)
    df_harvest = predict_daily_values(df_clean, HARVEST_TARGET, model, scaler_x, scaler_y, hp, feature_cols, device)
    
    status_text.text("ìµœì¢… ê²°ê³¼ ìƒì„± ì¤‘...")
    result = df_clean.merge(df_harvest, on=[KEY_FARM, KEY_DATE], how="left")
    
    # ì£¼ë³„ ì§‘ê³„
    weekly_results = []
    for farm in result[KEY_FARM].unique():
        farm_data = result[result[KEY_FARM] == farm].copy()
        farm_data['ë…„ë„'] = farm_data[KEY_DATE].dt.isocalendar().year
        farm_data['ì£¼ì°¨'] = farm_data[KEY_DATE].dt.isocalendar().week
        
        for (year, week), group in farm_data.groupby(['ë…„ë„', 'ì£¼ì°¨']):
            if HARVEST_TARGET in group.columns:
                mean_harvest = group[HARVEST_TARGET].mean() * 2
                weekly_results.append({
                    KEY_FARM: farm,
                    'ë…„ë„': int(year),
                    'ì£¼ì°¨': int(week),
                    HARVEST_TARGET: mean_harvest
                })
    
    df_weekly = pd.DataFrame(weekly_results).sort_values([KEY_FARM, 'ë…„ë„', 'ì£¼ì°¨'])
    
    progress_bar.progress(1.0)
    status_text.text("ì™„ë£Œ!")
    
    return df_weekly, removed

# ==================== Streamlit UI ====================
st.title("ğŸŒ± ì˜¨ì‹¤ ìƒìœ¡Â·ìˆ˜í™• ì˜ˆì¸¡ ì‹œìŠ¤í…œ")

# íƒ­ ìƒì„±
tab1, tab2, tab3 = st.tabs(["ğŸš€ ì˜ˆì¸¡", "ğŸ“Š 1ë‹¨ê³„: ìƒìœ¡ì§€í‘œ", "ğŸŒ¾ 2ë‹¨ê³„: ìˆ˜í™•ëŸ‰"])

# ==================== ì›í´ë¦­ íƒ­ ====================
with tab1:
    st.header("ìˆ˜í™•ëŸ‰ ì˜ˆì¸¡")
    st.info("í™˜ê²½ ë°ì´í„°ë§Œìœ¼ë¡œ ìƒìœ¡ì§€í‘œë¥¼ ë¨¼ì € ì˜ˆì¸¡í•œ í›„, ìˆ˜í™•ëŸ‰ê¹Œì§€ í•œ ë²ˆì— ì˜ˆì¸¡í•©ë‹ˆë‹¤.")
    
    col1, col2 = st.columns(2)
    
    with col1:
        env_file = st.file_uploader("ğŸ“ í™˜ê²½ ë°ì´í„° (í•„ìˆ˜)", type=['xlsx', 'xls'], key="direct_env")
    
    with col2:
        growth_file = st.file_uploader("ğŸ“ ìƒìœ¡ì§€í‘œ ì‹¤ì¸¡ê°’ (ì„ íƒ)", type=['xlsx', 'xls'], key="direct_growth")
    
    if st.button("ğŸš€ ì˜ˆì¸¡ ì‹œì‘", type="primary", use_container_width=True):
        if env_file is None:
            st.error("í™˜ê²½ ë°ì´í„° íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        else:
            try:
                with st.spinner("ë°ì´í„° ë¡œë“œ ì¤‘..."):
                    df_env = pd.read_excel(env_file)
                    df_env = sanitize_datetime_col(df_env, KEY_DATE)
                    
                    df_measured = None
                    if growth_file:
                        df_measured = pd.read_excel(growth_file)
                        df_measured = sanitize_datetime_col(df_measured, KEY_DATE)
                
                st.success(f"í™˜ê²½ ë°ì´í„°: {len(df_env)}í–‰")
                
                df_result, removed = process_direct(df_env, df_measured)
                
                st.success(f"âœ… ì˜ˆì¸¡ ì™„ë£Œ! (ê²°ì¸¡ì¹˜ {removed}í–‰ ì œê±°)")
                
                st.subheader("ğŸ“Š ì£¼ë³„ ìˆ˜í™•ëŸ‰ ì˜ˆì¸¡ ê²°ê³¼")
                st.dataframe(df_result, use_container_width=True, height=400)
                
                # í†µê³„
                col1, col2, col3 = st.columns(3)
                col1.metric("ì´ ì£¼ì°¨ ìˆ˜", len(df_result))
                col2.metric("í‰ê·  ìˆ˜í™•ëŸ‰", f"{df_result[HARVEST_TARGET].mean():.2f}")
                col3.metric("ìµœëŒ€ ìˆ˜í™•ëŸ‰", f"{df_result[HARVEST_TARGET].max():.2f}")
                
                # ë‹¤ìš´ë¡œë“œ
                output = io.BytesIO()
                df_result.to_excel(output, index=False, engine='openpyxl')
                output.seek(0)
                
                st.download_button(
                    label="ğŸ“¥ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ (Excel)",
                    data=output,
                    file_name=f"ì£¼ë³„ìˆ˜í™•ëŸ‰_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    use_container_width=True
                )
                
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

# ==================== 1ë‹¨ê³„ íƒ­ ====================
with tab2:
    st.header("1ë‹¨ê³„: ìƒìœ¡ì§€í‘œ ì˜ˆì¸¡")
    st.info("í™˜ê²½ ë°ì´í„°ë¥¼ ì…ë ¥í•˜ì—¬ 12ê°€ì§€ ìƒìœ¡ì§€í‘œë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.")
    
    col1, col2 = st.columns(2)
    
    with col1:
        env_file_s1 = st.file_uploader("ğŸ“ í™˜ê²½ ë°ì´í„° (í•„ìˆ˜)", type=['xlsx', 'xls'], key="step1_env")
    
    with col2:
        growth_file_s1 = st.file_uploader("ğŸ“ ìƒìœ¡ì§€í‘œ ì‹¤ì¸¡ê°’ (ì„ íƒ)", type=['xlsx', 'xls'], key="step1_growth")
    
    if st.button("ğŸ“Š ìƒìœ¡ì§€í‘œ ì˜ˆì¸¡", type="primary", use_container_width=True):
        if env_file_s1 is None:
            st.error("í™˜ê²½ ë°ì´í„° íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        else:
            try:
                with st.spinner("ë°ì´í„° ë¡œë“œ ì¤‘..."):
                    df_env = pd.read_excel(env_file_s1)
                    df_env = sanitize_datetime_col(df_env, KEY_DATE)
                    
                    df_measured = None
                    if growth_file_s1:
                        df_measured = pd.read_excel(growth_file_s1)
                        df_measured = sanitize_datetime_col(df_measured, KEY_DATE)
                
                df_result = process_step1(df_env, df_measured)
                
                st.success("âœ… ì˜ˆì¸¡ ì™„ë£Œ!")
                
                st.subheader("ğŸ“Š ìƒìœ¡ì§€í‘œ ì˜ˆì¸¡ ê²°ê³¼")
                st.dataframe(df_result, use_container_width=True, height=400)
                
                # ë‹¤ìš´ë¡œë“œ
                output = io.BytesIO()
                df_result.to_excel(output, index=False, engine='openpyxl')
                output.seek(0)
                
                st.download_button(
                    label="ğŸ“¥ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ (Excel)",
                    data=output,
                    file_name=f"ìƒìœ¡ì§€í‘œ_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    use_container_width=True
                )
                
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

# ==================== 2ë‹¨ê³„ íƒ­ ====================
with tab3:
    st.header("2ë‹¨ê³„: ìˆ˜í™•ëŸ‰ ì˜ˆì¸¡")
    st.info("í™˜ê²½ + ìƒìœ¡ì§€í‘œ ë°ì´í„°ë¥¼ ì…ë ¥í•˜ì—¬ ìˆ˜í™•ëŸ‰ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.")
    
    combined_file = st.file_uploader("ğŸ“ í™˜ê²½+ìƒìœ¡ í†µí•© ë°ì´í„°", type=['xlsx', 'xls'], key="step2_combined")
    
    if st.button("ğŸŒ¾ ìˆ˜í™•ëŸ‰ ì˜ˆì¸¡", type="primary", use_container_width=True):
        if combined_file is None:
            st.error("í†µí•© ë°ì´í„° íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        else:
            try:
                with st.spinner("ë°ì´í„° ë¡œë“œ ì¤‘..."):
                    df_combined = pd.read_excel(combined_file)
                    df_combined = sanitize_datetime_col(df_combined, KEY_DATE)
                
                df_result, removed = process_step2(df_combined)
                
                st.success(f"âœ… ì˜ˆì¸¡ ì™„ë£Œ! (ê²°ì¸¡ì¹˜ {removed}í–‰ ì œê±°)")
                
                st.subheader("ğŸ“Š ìˆ˜í™•ëŸ‰ ì˜ˆì¸¡ ê²°ê³¼")
                st.dataframe(df_result, use_container_width=True, height=400)
                
                # í†µê³„
                if HARVEST_TARGET in df_result.columns:
                    col1, col2, col3 = st.columns(3)
                    col1.metric("í‰ê·  ìˆ˜í™•ëŸ‰", f"{df_result[HARVEST_TARGET].mean():.2f}")
                    col2.metric("ìµœì†Œ ìˆ˜í™•ëŸ‰", f"{df_result[HARVEST_TARGET].min():.2f}")
                    col3.metric("ìµœëŒ€ ìˆ˜í™•ëŸ‰", f"{df_result[HARVEST_TARGET].max():.2f}")
                
                # ë‹¤ìš´ë¡œë“œ
                output = io.BytesIO()
                df_result.to_excel(output, index=False, engine='openpyxl')
                output.seek(0)
                
                st.download_button(
                    label="ğŸ“¥ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ (Excel)",
                    data=output,
                    file_name=f"ìˆ˜í™•ëŸ‰_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    use_container_width=True
                )
                
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("â„¹ï¸ ì‚¬ìš© ë°©ë²•")
    
    st.markdown("""
    ### ì›í´ë¦­ ì˜ˆì¸¡
    í™˜ê²½ ë°ì´í„°ë§Œìœ¼ë¡œ ìƒìœ¡ì§€í‘œì™€ ìˆ˜í™•ëŸ‰ì„ í•œ ë²ˆì— ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    
    ### 1ë‹¨ê³„: ìƒìœ¡ì§€í‘œ ì˜ˆì¸¡
    í™˜ê²½ ë°ì´í„°ë¡œ 12ê°€ì§€ ìƒìœ¡ì§€í‘œë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    
    ### 2ë‹¨ê³„: ìˆ˜í™•ëŸ‰ ì˜ˆì¸¡
    1ë‹¨ê³„ ê²°ê³¼ ë˜ëŠ” ì‹¤ì¸¡ ìƒìœ¡ì§€í‘œë¡œ ìˆ˜í™•ëŸ‰ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    
    ---
    
    ### ğŸ“ í•„ìš”í•œ ë°ì´í„°
    - **í™˜ê²½ ë°ì´í„°**: ì˜¨ì‹¤ í™˜ê²½ ì¸¡ì •ê°’ (ì˜¨ë„, ìŠµë„ ë“±)
    - **ìƒìœ¡ì§€í‘œ**: ì‹¤ì¸¡ê°’ (ì„ íƒì‚¬í•­, ìˆìœ¼ë©´ ì •í™•ë„ í–¥ìƒ)
    
    ### ğŸ“Š ì˜ˆì¸¡ í•­ëª©
    - **ìƒìœ¡ì§€í‘œ**: ê°œí™”ë§ˆë””, ì°©ê³¼ë§ˆë””, ìˆ˜í™•ë§ˆë”” ë“± 12ê°œ
    - **ìˆ˜í™•ëŸ‰**: ì£¼ë‹¹ëˆ„ì ìˆ˜í™•ìˆ˜
    """)
    
    st.markdown("---")
    st.markdown("ğŸŒ± **ì˜¨ì‹¤ ìƒìœ¡Â·ìˆ˜í™• ì˜ˆì¸¡ ì‹œìŠ¤í…œ**")
    st.markdown("Powered by GRU Deep Learning")
